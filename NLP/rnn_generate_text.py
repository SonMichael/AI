# -*- coding: utf-8 -*-
"""RNN_Generate_Text.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y_Pta7jwitNbOk99k7QYFdL5_ldkbJr2

## 1. Tải dữ liệu
"""

!wget --no-check-certificate \
    https://storage.googleapis.com/protonx-cloud-storage/data.txt
data = open('data.txt').read()

"""## 2. Thêm các thư viện cần thiết"""

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers
import tensorflow.keras.utils as ku 
import numpy as np

"""## 3. Xử lý dữ liệu"""

corpus = data.lower().split("\n")

corpus[:4]

"""### 3.1. Xây dựng tokenizer"""

tokenizer = Tokenizer()
tokenizer.fit_on_texts(corpus)
total_words = len(tokenizer.word_index) + 1
# OOV

"""### 3.2. Tách câu

Tách từng câu thành nhiều phần có chiều dài tăng dần để làm từng điểm dữ liệu
"""

input_sequences = []
for line in corpus:
  token_list = tokenizer.texts_to_sequences([line])[0]
  for i in range(1, len(token_list)):
    n_gram_sequence = token_list[:i+1]
    input_sequences.append(n_gram_sequence)

input_sequences[:10]

tokenizer.sequences_to_texts([[34, 417]])

"""Thực chất các câu sẽ được cắt thành nhiều phần như thế này"""

for point in input_sequences[:10]:
  print(" ".join(tokenizer.sequences_to_texts([point])))

"""### 3.3. Chia features, label

Thực hiện Padding
"""

# pad sequences 
max_sequence_len = max([len(x) for x in input_sequences])
input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))

"""Các câu được tách bên trên sẽ được padding với việc chèn các giá trị 0 vào trước câu để tạo ra các câu có chiều dài bằng nhau.

Việc **padding vào trước** vì bài toán này ta sẽ sinh từ từ phía sau nên thông tin bên phải là những giá trị khác 0. 
"""

input_sequences[:10]

"""Cắt cột cuối cùng của ma trận bên trên để làm nhãn"""

predictors, label = input_sequences[:,:-1],input_sequences[:,-1]
# Chuyển thành One Hot vector

label[i:i+1]

for i in range(10):
  print("{} ---> {}".format(" ".join(tokenizer.sequences_to_texts(predictors[i:i+1])), tokenizer.sequences_to_texts([label[i:i+1]])[0]))

"""Chuyển từng nhãn thành vector one hot với số lượng là số từ trong từ điển"""

label = ku.to_categorical(label, num_classes=total_words)

"""## 4. Xây dựng model

Mô hình này có nhiệm vụ học khả năng một từ hiện tại có xác suất xuất hiện sau một số lượng từ nhất định như thế nào.

Mạng bao gồm:

- 1 lớp Embedding với chiều embedding là 100
- Một lớp 1 Bidirectional với cell LSTM 150 node
- Một lớp LSTM với 100 node
- Mạng nơ ron phân loại gồm một lớp ẩn
"""

model = Sequential()

model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))

model.add(Bidirectional(LSTM(150, return_sequences = True)))

model.add(Dropout(0.2))

model.add(LSTM(100)) # h

model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))

model.add(Dense(total_words, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

print(model.summary())

"""Tiến hành training"""

history = model.fit(predictors, label, epochs=130, verbose=0)

"""### 5. Dự đoán 10 từ tiếp theo

Câu mồi
"""

# test_seq = 'Thank you for'

# 'Thank you for' -> supporting

# 'Thank you for supporting' -> me

"""Từ câu mồi
- Dự đoán ra từ tiếp theo từ các từ của câu hiện tại
- Nối từ đã được dự đoán vào câu hiện tại
- Tiếp tục dùng câu hiện tại này để dự đoán
- Thực hiện đến khi chiều dài của câu đạt một giới hạn cụ thể hoặc số từ sinh ra đạt giới hạn
"""

next_words = 10

for _ in range(next_words):
  # Chuyển câu thành vector
  token_list = tokenizer.texts_to_sequences([test_seq])[0]
  
  # Padding câu
  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
  
  # Dự đoán từ tiếp theo
  predicted = model.predict(token_list, verbose=0)
  
  output_word = ""

  predicted_id = np.argmax(predicted)

  if predicted_id in tokenizer.index_word:
    output_word = tokenizer.index_word[predicted_id]
    test_seq += " " + output_word
  else:
    break
  
print(test_seq)

"""Để sinh tốt hơn bạn có thể tham khảo thêm thuật toán **Beam Search**"""

# despite of wrinkles
# this: 0.6
# that: 0.54